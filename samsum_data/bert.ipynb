{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4262d84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment optimized for better performance\n"
     ]
    }
   ],
   "source": [
    "# Fix compatibility issues and optimize performance\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set environment variables for better performance\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['TRANSFORMERS_CACHE'] = './model_cache'\n",
    "\n",
    "print(\"✅ Environment optimized for better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1ca31",
   "metadata": {},
   "source": [
    "# Small Model  T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f214fd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading optimized summarization model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb59f1558fc48f3b6193a1f57636f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f3f2b8521c4156bb9618deb4431609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/460M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc436bff2774749a9f18f40c72aeac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/460M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ DistilBART not available, falling back to T5: Could not load model sshleifer/distilbart-cnn-6-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>, <class 'transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration'>). See the original errors:\n",
      "\n",
      "while loading with AutoModelForSeq2SeqLM, an error is thrown:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 311, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 4747, in from_pretrained\n",
      "    config, torch_dtype, dtype_orig = _get_torch_dtype(\n",
      "                                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 1345, in _get_torch_dtype\n",
      "    state_dict = load_state_dict(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 556, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py\", line 1517, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **fp32_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 311, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 4839, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 5105, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 556, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py\", line 1517, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "while loading with TFAutoModelForSeq2SeqLM, an error is thrown:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_tf_utils.py\", line 2857, in from_pretrained\n",
      "    raise OSError(\n",
      "OSError: sshleifer/distilbart-cnn-6-6 does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **fp32_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_tf_utils.py\", line 2857, in from_pretrained\n",
      "    raise OSError(\n",
      "OSError: sshleifer/distilbart-cnn-6-6 does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n",
      "\n",
      "while loading with BartForConditionalGeneration, an error is thrown:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 311, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 4747, in from_pretrained\n",
      "    config, torch_dtype, dtype_orig = _get_torch_dtype(\n",
      "                                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 1345, in _get_torch_dtype\n",
      "    state_dict = load_state_dict(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 556, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py\", line 1517, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **fp32_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 311, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 4839, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 5105, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 556, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py\", line 1517, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "while loading with TFBartForConditionalGeneration, an error is thrown:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_tf_utils.py\", line 2857, in from_pretrained\n",
      "    raise OSError(\n",
      "OSError: sshleifer/distilbart-cnn-6-6 does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **fp32_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_tf_utils.py\", line 2857, in from_pretrained\n",
      "    raise OSError(\n",
      "OSError: sshleifer/distilbart-cnn-6-6 does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edd91900c394af0b1d8d84dab63301c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbd333e87914bf9b737b9e8684daf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba215d787aa4402a9621b3e81f2a2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4d8a5aa15a4d2d872463c1e8087931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/460M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7eb53619c67454099455b3538b5c65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93c56619a574b09a9d656bc9a3af187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2639fd2233174f518d95e73eff12d4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded in 27.72 seconds: T5-Small (Fallback)\n",
      "📊 Processing 5 samples for performance testing...\n",
      "\n",
      "📋 Sample 1 (processed in 3.51s):\n",
      "Original: Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't...\n",
      "Generated: Amanda: Lemme check Hannah: file_gif> Amanda: Sorry, can't find it. he called her last time we were at the park together Hannah: I don't know him well.\n",
      "Reference: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "\n",
      "📋 Sample 2 (processed in 2.26s):\n",
      "Original: Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it'...\n",
      "Generated: Rob: I know! I especially like the train part! Rob: Hahaha! No one talks to the machine like that! I'll watch some of his stand-ups on youtube.\n",
      "Reference: Eric and Rob are going to watch a stand-up on youtube.\n",
      "\n",
      "📋 Sample 3 (processed in 2.12s):\n",
      "Original: Lenny: Babe, can you help me with something?\n",
      "Bob: Sure, what's up?\n",
      "Lenny: Which one should I pick?\n",
      "B...\n",
      "Generated: Bob: what matters is what you'll give you the most outfit options. Bob: pick the best quality then Lenny: ur right, thx Bob: no prob.\n",
      "Reference: Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.\n",
      "\n",
      "📋 Sample 4 (processed in 2.96s):\n",
      "Original: Will: hey babe, what do you want for dinner tonight?\n",
      "Emma:  gah, don't even worry about it tonight\n",
      "W...\n",
      "Generated: Emma: hey babe, what do you want for dinner tonight? Emma: gah, don't worry about cooking though, I'm not hungry. Emma: no no it's alright, love you too\n",
      "Reference: Emma will be home soon and she will let Will know.\n",
      "\n",
      "📋 Sample 5 (processed in 2.66s):\n",
      "Original: Ollie: Hi , are you in Warsaw\n",
      "Jane: yes, just back! Btw are you free for diner the 19th?\n",
      "Ollie: nope...\n",
      "Generated: Ollie: we have lunch this week and you must be there, remember? Jane: ok, we don't have any more whisky! lol. oh, ok for tea!\n",
      "Reference: Jane is in Warsaw. Ollie and Jane has a party. Jane lost her calendar. They will get a lunch this week on Friday. Ollie accidentally called Jane and talked about whisky. Jane cancels lunch. They'll meet for a tea at 6 pm.\n",
      "\n",
      "⏱️ Performance Summary:\n",
      "Total time: 13.51 seconds\n",
      "Average per sample: 2.70 seconds\n",
      "Samples per minute: 22.2\n",
      "Model used: T5-Small (Fallback)\n"
     ]
    }
   ],
   "source": [
    "# Optimized Fast Summarization with Smaller Models\n",
    "import json\n",
    "from transformers import pipeline, set_seed\n",
    "import time\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "print(\"🚀 Loading optimized summarization model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Use a smaller, faster model for better performance\n",
    "try:\n",
    "    # Option 1: Faster BART model\n",
    "    summarizer = pipeline(\n",
    "        \"summarization\", \n",
    "        model=\"sshleifer/distilbart-cnn-6-6\",  # Smaller, faster version of BART\n",
    "        device=-1,  # Force CPU usage (set to 0 for GPU if available)\n",
    "        batch_size=1,\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "    model_name = \"DistilBART (Optimized)\"\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ DistilBART not available, falling back to T5: {e}\")\n",
    "    # Fallback to T5-small\n",
    "    summarizer = pipeline(\n",
    "        \"summarization\",\n",
    "        model=\"t5-small\",\n",
    "        device=-1,\n",
    "        batch_size=1,\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "    model_name = \"T5-Small (Fallback)\"\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"✅ Model loaded in {load_time:.2f} seconds: {model_name}\")\n",
    "\n",
    "# Load test data\n",
    "with open(\"test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# Use first 5 samples for faster testing\n",
    "samples = data[:5]\n",
    "print(f\"📊 Processing {len(samples)} samples for performance testing...\")\n",
    "\n",
    "# Process samples with timing\n",
    "total_start = time.time()\n",
    "results = []\n",
    "\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    sample_start = time.time()\n",
    "    dialogue = sample[\"dialogue\"]\n",
    "    \n",
    "    # Optimized summarization parameters\n",
    "    try:\n",
    "        summary_result = summarizer(\n",
    "            dialogue,\n",
    "            max_length=50,      # Reduced for speed\n",
    "            min_length=10,\n",
    "            do_sample=False,    # Deterministic output\n",
    "            truncation=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        summary = summary_result[0][\"summary_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing sample {i}: {e}\")\n",
    "        summary = \"Error in summarization\"\n",
    "    \n",
    "    sample_time = time.time() - sample_start\n",
    "    \n",
    "    print(f\"\\n📋 Sample {i} (processed in {sample_time:.2f}s):\")\n",
    "    print(f\"Original: {dialogue[:100]}...\")\n",
    "    print(f\"Generated: {summary}\")\n",
    "    print(f\"Reference: {sample['summary']}\")\n",
    "    \n",
    "    results.append({\n",
    "        'sample': i,\n",
    "        'processing_time': sample_time,\n",
    "        'generated_summary': summary,\n",
    "        'reference_summary': sample['summary']\n",
    "    })\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "avg_time = total_time / len(samples)\n",
    "\n",
    "print(f\"\\n⏱️ Performance Summary:\")\n",
    "print(f\"Total time: {total_time:.2f} seconds\")\n",
    "print(f\"Average per sample: {avg_time:.2f} seconds\")\n",
    "print(f\"Samples per minute: {60/avg_time:.1f}\")\n",
    "print(f\"Model used: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e2eba4",
   "metadata": {},
   "source": [
    "# Fixing NumPy Compatibility Issues With BERT Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ed386fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "Original Dialogue: Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "Model Summary   : Hannah asks Amanda for Betty's number. Amanda can't find it. Hannah asks Larry. Amanda asks Larry to call Betty.\n",
      "Reference Summary: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "\n",
      "Sample 2:\n",
      "Original Dialogue: Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it's really funny!\n",
      "Eric: I know! I especially like the train part!\n",
      "Rob: Hahaha! No one talks to the machine like that!\n",
      "Eric: Is this his only stand-up?\n",
      "Rob: Idk. I'll check.\n",
      "Eric: Sure.\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\n",
      "Eric: Gr8! I'll watch them now!\n",
      "Rob: Me too!\n",
      "Eric: MACHINE!\n",
      "Rob: MACHINE!\n",
      "Eric: TTYL?\n",
      "Rob: Sure :)\n",
      "Model Summary   : Eric: Is this his only stand-up? I'll check. There are some of his stand-ups on youtube.\n",
      "Reference Summary: Eric and Rob are going to watch a stand-up on youtube.\n",
      "\n",
      "Sample 3:\n",
      "Original Dialogue: Lenny: Babe, can you help me with something?\n",
      "Bob: Sure, what's up?\n",
      "Lenny: Which one should I pick?\n",
      "Bob: Send me photos\n",
      "Lenny:  <file_photo>\n",
      "Lenny:  <file_photo>\n",
      "Lenny:  <file_photo>\n",
      "Bob: I like the first ones best\n",
      "Lenny: But I already have purple trousers. Does it make sense to have two pairs?\n",
      "Bob: I have four black pairs :D :D\n",
      "Lenny: yeah, but shouldn't I pick a different color?\n",
      "Bob: what matters is what you'll give you the most outfit options\n",
      "Lenny: So I guess I'll buy the first or the third pair then\n",
      "Bob: Pick the best quality then\n",
      "Lenny: ur right, thx\n",
      "Bob: no prob :)\n",
      "Model Summary   : Lenny: Babe, can you help me with something? Bob: Sure, what's up? Lenny: Which one should I pick?Bob: Send me photos. Lenny says he'll buy the first or the third pair.\n",
      "Reference Summary: Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.\n",
      "\n",
      "Sample 4:\n",
      "Original Dialogue: Will: hey babe, what do you want for dinner tonight?\n",
      "Emma:  gah, don't even worry about it tonight\n",
      "Will: what do you mean? everything ok?\n",
      "Emma: not really, but it's ok, don't worry about cooking though, I'm not hungry\n",
      "Will: Well what time will you be home?\n",
      "Emma: soon, hopefully\n",
      "Will: you sure? Maybe you want me to pick you up?\n",
      "Emma: no no it's alright. I'll be home soon, i'll tell you when I get home. \n",
      "Will: Alright, love you. \n",
      "Emma: love you too. \n",
      "Model Summary   : Emma:  gah, don't even worry about it tonight. Will:  Maybe you want me to pick you up? Emma: no no it's alright. I'll be home soon.\n",
      "Reference Summary: Emma will be home soon and she will let Will know.\n",
      "\n",
      "Sample 5:\n",
      "Original Dialogue: Ollie: Hi , are you in Warsaw\n",
      "Jane: yes, just back! Btw are you free for diner the 19th?\n",
      "Ollie: nope!\n",
      "Jane: and the  18th?\n",
      "Ollie: nope, we have this party and you must be there, remember?\n",
      "Jane: oh right! i lost my calendar..  thanks for reminding me\n",
      "Ollie: we have lunch this week?\n",
      "Jane: with pleasure!\n",
      "Ollie: friday?\n",
      "Jane: ok\n",
      "Jane: what do you mean \" we don't have any more whisky!\" lol..\n",
      "Ollie: what!!!\n",
      "Jane: you just call me and the all thing i heard was that sentence about whisky... what's wrong with you?\n",
      "Ollie: oh oh... very strange! i have to be carefull may be there is some spy in my mobile! lol\n",
      "Jane: dont' worry, we'll check on friday.\n",
      "Ollie: don't forget to bring some sun with you\n",
      "Jane: I can't wait to be in Morocco..\n",
      "Ollie: enjoy and see you friday\n",
      "Jane: sorry Ollie, i'm very busy, i won't have time for lunch  tomorrow, but may be at 6pm after my courses?this trip to Morocco was so nice, but time consuming!\n",
      "Ollie: ok for tea!\n",
      "Jane: I'm on my way..\n",
      "Ollie: tea is ready, did you bring the pastries?\n",
      "Jane: I already ate them all... see you in a minute\n",
      "Ollie: ok\n",
      "Model Summary   : Ollie calls his friend in Warsaw, Poland. He is in the middle of a week-long trip to Morocco. Ollie asks if he can join him for lunch.\n",
      "Reference Summary: Jane is in Warsaw. Ollie and Jane has a party. Jane lost her calendar. They will get a lunch this week on Friday. Ollie accidentally called Jane and talked about whisky. Jane cancels lunch. They'll meet for a tea at 6 pm.\n",
      "\n",
      "Sample 6:\n",
      "Original Dialogue: Benjamin: Hey guys, what are we doing with the keys today?\n",
      "Hilary: I've got them. Whoever wants them can meet me at lunchtime or after\n",
      "Elliot: I'm ok. We're meeting for the drinks in the evening anyway and I guess we'll be going back to the apartment together?\n",
      "Hilary: Yeah, I guess so\n",
      "Daniel: I'm with Hilary atm and won't let go of her for the rest of the day, so any option you guys choose is good for me\n",
      "Benjamin: Hmm I might actually pass by at lunchtime, take the keys and go take a nap. I'm sooo tired after yesterday\n",
      "Hilary: Sounds good. We'll be having lunch with some French people (the ones who work on the history of food in colonial Mexico - I already see you yawning your head off)\n",
      "Benjamin: YAAAAWN 🙊 Where and where are you meeting?\n",
      "Hilary: So I'm meeting them at the entrance to the conference hall at 2 pm and then we'll head to this place called La Cantina. Italian cuisine, which is quite funny, but that's what they've chosen\n",
      "Benjamin: Interesting 😱 To be honest, Hilary, I almost feel like changing my mind. Wanting to take this nap might end up costing me to dear\n",
      "Hilary: Oh come on 😂\n",
      "Benjamin: All these terrible obstacles on mu way to bed might just prove to much to take\n",
      "Hilary: We'll try to avoid talking about their subject of research. Oh wait, no, I'm actually meeting them because I wanted to chat about their research lol\n",
      "Elliot: 🙉\n",
      "Hilary: Do join us, we're going to have fun. And then you'll take the keys and take this most deserved of naps\n",
      "Elliot: Sounds like a plan 😂\n",
      "Hilary: 😎\n",
      "Elliot: See you at 2 then xx\n",
      "Model Summary   : Hilary: I've got the keys. Whoever wants them can meet me at lunchtime or after. We'll be having lunch with some French people (the ones who work on the history of food in colonial Mexico - I already see you yawning your head off)\n",
      "Reference Summary: Hilary has the keys to the apartment. Benjamin wants to get them and go take a nap. Hilary is having lunch with some French people at La Cantina. Hilary is meeting them at the entrance to the conference hall at 2 pm. Benjamin and Elliot might join them. They're meeting for the drinks in the evening.\n",
      "\n",
      "Sample 7:\n",
      "Original Dialogue: Max: Know any good sites to buy clothes from?\n",
      "Payton: Sure :) <file_other> <file_other> <file_other> <file_other> <file_other> <file_other> <file_other>\n",
      "Max: That's a lot of them!\n",
      "Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\n",
      "Max: I'll check them out. Thanks. \n",
      "Payton: No problem :)\n",
      "Max: How about u?\n",
      "Payton: What about me?\n",
      "Max: Do u like shopping?\n",
      "Payton: Yes and no.\n",
      "Max: How come?\n",
      "Payton: I like browsing, trying on, looking in the mirror and seeing how I look, but not always buying.\n",
      "Max: Y not?\n",
      "Payton: Isn't it obvious? ;)\n",
      "Max: Sry ;)\n",
      "Payton: If I bought everything I liked, I'd have nothing left to live on ;)\n",
      "Max: Same here, but probably different category ;)\n",
      "Payton: Lol\n",
      "Max: So what do u usually buy?\n",
      "Payton: Well, I have 2 things I must struggle to resist!\n",
      "Max: Which are?\n",
      "Payton: Clothes, ofc ;)\n",
      "Max: Right. And the second one?\n",
      "Payton: Books. I absolutely love reading!\n",
      "Max: Gr8! What books do u read?\n",
      "Payton: Everything I can get my hands on :)\n",
      "Max: Srsly?\n",
      "Payton: Yup :)\n",
      "Model Summary   : \"I like browsing, trying on, looking in the mirror and seeing how I look, but not always buying\" \"If I bought everything I liked, I'd have nothing left to live on\" \"I absolutely love reading! I must struggle to resist!\"\n",
      "Reference Summary: Payton provides Max with websites selling clothes. Payton likes browsing and trying on the clothes but not necessarily buying them. Payton usually buys clothes and books as he loves reading.\n",
      "\n",
      "Sample 8:\n",
      "Original Dialogue: Rita: I'm so bloody tired. Falling asleep at work. :-(\n",
      "Tina: I know what you mean.\n",
      "Tina: I keep on nodding off at my keyboard hoping that the boss doesn't notice..\n",
      "Rita: The time just keeps on dragging on and on and on.... \n",
      "Rita: I keep on looking at the clock and there's still 4 hours of this drudgery to go.\n",
      "Tina: Times like these I really hate my work.\n",
      "Rita: I'm really not cut out for this level of boredom.\n",
      "Tina: Neither am I.\n",
      "Model Summary   : Rita: I'm really not cut out for this level of boredom. Tina: Neither am I.\n",
      "Reference Summary: Rita and Tina are bored at work and have still 4 hours left.\n",
      "\n",
      "Sample 9:\n",
      "Original Dialogue: Beatrice: I am in town, shopping. They have nice scarfs in the shop next to the church. Do you want one?\n",
      "Leo: No, thanks\n",
      "Beatrice: But you don't have a scarf.\n",
      "Leo: Because I don't need it.\n",
      "Beatrice: Last winter you had a cold all the time. A scarf could help.\n",
      "Leo: I don't like them.\n",
      "Beatrice: Actually, I don't care. You will get a scarf.\n",
      "Leo: How understanding of you!\n",
      "Beatrice: You were complaining the whole winter that you're going to die. I've had enough.\n",
      "Leo: Eh.\n",
      "Model Summary   : Leo was shopping when his wife, Beatrice, asked him for a scarf. He said he didn't need one because he had a cold all the time. Beatrice said she would get him a scarf if he wanted one.\n",
      "Reference Summary: Beatrice wants to buy Leo a scarf, but he doesn't like scarves. She cares about his health and will buy him a scarf no matter his opinion.\n",
      "\n",
      "Sample 10:\n",
      "Original Dialogue: Ivan: hey eric\n",
      "Eric: yeah man\n",
      "Ivan: so youre coming to the wedding\n",
      "Eric: your brother's\n",
      "Ivan: yea\n",
      "Eric: i dont know mannn\n",
      "Ivan: YOU DONT KNOW??\n",
      "Eric: i just have a lot to do at home, plus i dont know if my parents would let me\n",
      "Ivan: ill take care of your parents\n",
      "Eric: youre telling me you have the guts to talk to them XD\n",
      "Ivan: thats my problem\n",
      "Eric: okay man, if you say so\n",
      "Ivan: yea just be there \n",
      "Eric: alright\n",
      "Model Summary   : Ivan and Eric are at the wedding of Ivan's brother. Ivan asks Eric if he is going to the wedding. Eric says he doesn't know if his parents would let him. Ivan says he will take care of his parents.\n",
      "Reference Summary: Eric doesn't know if his parents let him go to Ivan's brother's wedding. Ivan will talk to them.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load test data from test.json\n",
    "with open(\"test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# Use only the first 10 dialogues\n",
    "samples = data[:10]\n",
    "\n",
    "# Load summarization pipeline using Facebook's BART model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Summarize and print results\n",
    "for i, sample in enumerate(samples):\n",
    "    dialogue = sample[\"dialogue\"]\n",
    "    summary = summarizer(dialogue, max_length=60, min_length=10, do_sample=False)[0][\"summary_text\"]\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(\"Original Dialogue:\", dialogue)\n",
    "    print(\"Model Summary   :\", summary)\n",
    "    print(\"Reference Summary:\", sample[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a23ead",
   "metadata": {},
   "source": [
    "# Using the SAMSum Dataset with Pegasus Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7673c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "Original Dialogue: Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "Model Summary    : Amanda can't find Betty's number. Larry called her last time they were at the park together.\n",
      "Reference Summary: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "\n",
      "Sample 2:\n",
      "Original Dialogue: Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it's really funny!\n",
      "Eric: I know! I especially like the train part!\n",
      "Rob: Hahaha! No one talks to the machine like that!\n",
      "Eric: Is this his only stand-up?\n",
      "Rob: Idk. I'll check.\n",
      "Eric: Sure.\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\n",
      "Eric: Gr8! I'll watch them now!\n",
      "Rob: Me too!\n",
      "Eric: MACHINE!\n",
      "Rob: MACHINE!\n",
      "Eric: TTYL?\n",
      "Rob: Sure :)\n",
      "Model Summary    : Eric, Rob and Rob will watch the Russian comedian's stand-up on YouTube.\n",
      "Reference Summary: Eric and Rob are going to watch a stand-up on youtube.\n",
      "\n",
      "Sample 3:\n",
      "Original Dialogue: Lenny: Babe, can you help me with something?\n",
      "Bob: Sure, what's up?\n",
      "Lenny: Which one should I pick?\n",
      "Bob: Send me photos\n",
      "Lenny:  <file_photo>\n",
      "Lenny:  <file_photo>\n",
      "Lenny:  <file_photo>\n",
      "Bob: I like the first ones best\n",
      "Lenny: But I already have purple trousers. Does it make sense to have two pairs?\n",
      "Bob: I have four black pairs :D :D\n",
      "Lenny: yeah, but shouldn't I pick a different color?\n",
      "Bob: what matters is what you'll give you the most outfit options\n",
      "Lenny: So I guess I'll buy the first or the third pair then\n",
      "Bob: Pick the best quality then\n",
      "Lenny: ur right, thx\n",
      "Bob: no prob :)\n",
      "Model Summary    : Lenny wants to buy purple trousers. Bob likes the first ones best. Lenny already has purple trousers.\n",
      "Reference Summary: Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.\n",
      "\n",
      "Sample 4:\n",
      "Original Dialogue: Will: hey babe, what do you want for dinner tonight?\n",
      "Emma:  gah, don't even worry about it tonight\n",
      "Will: what do you mean? everything ok?\n",
      "Emma: not really, but it's ok, don't worry about cooking though, I'm not hungry\n",
      "Will: Well what time will you be home?\n",
      "Emma: soon, hopefully\n",
      "Will: you sure? Maybe you want me to pick you up?\n",
      "Emma: no no it's alright. I'll be home soon, i'll tell you when I get home. \n",
      "Will: Alright, love you. \n",
      "Emma: love you too. \n",
      "Model Summary    : Emma will be home soon. She will tell Will when she gets home.\n",
      "Reference Summary: Emma will be home soon and she will let Will know.\n",
      "\n",
      "Sample 5:\n",
      "Original Dialogue: Ollie: Hi , are you in Warsaw\n",
      "Jane: yes, just back! Btw are you free for diner the 19th?\n",
      "Ollie: nope!\n",
      "Jane: and the  18th?\n",
      "Ollie: nope, we have this party and you must be there, remember?\n",
      "Jane: oh right! i lost my calendar..  thanks for reminding me\n",
      "Ollie: we have lunch this week?\n",
      "Jane: with pleasure!\n",
      "Ollie: friday?\n",
      "Jane: ok\n",
      "Jane: what do you mean \" we don't have any more whisky!\" lol..\n",
      "Ollie: what!!!\n",
      "Jane: you just call me and the all thing i heard was that sentence about whisky... what's wrong with you?\n",
      "Ollie: oh oh... very strange! i have to be carefull may be there is some spy in my mobile! lol\n",
      "Jane: dont' worry, we'll check on friday.\n",
      "Ollie: don't forget to bring some sun with you\n",
      "Jane: I can't wait to be in Morocco..\n",
      "Ollie: enjoy and see you friday\n",
      "Jane: sorry Ollie, i'm very busy, i won't have time for lunch  tomorrow, but may be at 6pm after my courses?this trip to Morocco was so nice, but time consuming!\n",
      "Ollie: ok for tea!\n",
      "Jane: I'm on my way..\n",
      "Ollie: tea is ready, did you bring the pastries?\n",
      "Jane: I already ate them all... see you in a minute\n",
      "Ollie: ok\n",
      "Model Summary    : Jane is back in Warsaw. She lost her calendar. She's on her way to Morocco. They'll check on Friday.\n",
      "Reference Summary: Jane is in Warsaw. Ollie and Jane has a party. Jane lost her calendar. They will get a lunch this week on Friday. Ollie accidentally called Jane and talked about whisky. Jane cancels lunch. They'll meet for a tea at 6 pm.\n",
      "\n",
      "Sample 6:\n",
      "Original Dialogue: Benjamin: Hey guys, what are we doing with the keys today?\n",
      "Hilary: I've got them. Whoever wants them can meet me at lunchtime or after\n",
      "Elliot: I'm ok. We're meeting for the drinks in the evening anyway and I guess we'll be going back to the apartment together?\n",
      "Hilary: Yeah, I guess so\n",
      "Daniel: I'm with Hilary atm and won't let go of her for the rest of the day, so any option you guys choose is good for me\n",
      "Benjamin: Hmm I might actually pass by at lunchtime, take the keys and go take a nap. I'm sooo tired after yesterday\n",
      "Hilary: Sounds good. We'll be having lunch with some French people (the ones who work on the history of food in colonial Mexico - I already see you yawning your head off)\n",
      "Benjamin: YAAAAWN 🙊 Where and where are you meeting?\n",
      "Hilary: So I'm meeting them at the entrance to the conference hall at 2 pm and then we'll head to this place called La Cantina. Italian cuisine, which is quite funny, but that's what they've chosen\n",
      "Benjamin: Interesting 😱 To be honest, Hilary, I almost feel like changing my mind. Wanting to take this nap might end up costing me to dear\n",
      "Hilary: Oh come on 😂\n",
      "Benjamin: All these terrible obstacles on mu way to bed might just prove to much to take\n",
      "Hilary: We'll try to avoid talking about their subject of research. Oh wait, no, I'm actually meeting them because I wanted to chat about their research lol\n",
      "Elliot: 🙉\n",
      "Hilary: Do join us, we're going to have fun. And then you'll take the keys and take this most deserved of naps\n",
      "Elliot: Sounds like a plan 😂\n",
      "Hilary: 😎\n",
      "Elliot: See you at 2 then xx\n",
      "Model Summary    : Hilary has the keys for the keys today. Elliot and Daniel are meeting for drinks in the evening. Benjamin might pass by at lunchtime and take the keys and take a nap. Hilary is meeting French people at La Cantina at 2 pm.\n",
      "Reference Summary: Hilary has the keys to the apartment. Benjamin wants to get them and go take a nap. Hilary is having lunch with some French people at La Cantina. Hilary is meeting them at the entrance to the conference hall at 2 pm. Benjamin and Elliot might join them. They're meeting for the drinks in the evening.\n",
      "\n",
      "Sample 7:\n",
      "Original Dialogue: Max: Know any good sites to buy clothes from?\n",
      "Payton: Sure :) <file_other> <file_other> <file_other> <file_other> <file_other> <file_other> <file_other>\n",
      "Max: That's a lot of them!\n",
      "Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\n",
      "Max: I'll check them out. Thanks. \n",
      "Payton: No problem :)\n",
      "Max: How about u?\n",
      "Payton: What about me?\n",
      "Max: Do u like shopping?\n",
      "Payton: Yes and no.\n",
      "Max: How come?\n",
      "Payton: I like browsing, trying on, looking in the mirror and seeing how I look, but not always buying.\n",
      "Max: Y not?\n",
      "Payton: Isn't it obvious? ;)\n",
      "Max: Sry ;)\n",
      "Payton: If I bought everything I liked, I'd have nothing left to live on ;)\n",
      "Max: Same here, but probably different category ;)\n",
      "Payton: Lol\n",
      "Max: So what do u usually buy?\n",
      "Payton: Well, I have 2 things I must struggle to resist!\n",
      "Max: Which are?\n",
      "Payton: Clothes, ofc ;)\n",
      "Max: Right. And the second one?\n",
      "Payton: Books. I absolutely love reading!\n",
      "Max: Gr8! What books do u read?\n",
      "Payton: Everything I can get my hands on :)\n",
      "Max: Srsly?\n",
      "Payton: Yup :)\n",
      "Model Summary    : Max will check out some good sites to buy clothes from. Payton will buy clothes from 2 or 3 of them.\n",
      "Reference Summary: Payton provides Max with websites selling clothes. Payton likes browsing and trying on the clothes but not necessarily buying them. Payton usually buys clothes and books as he loves reading.\n",
      "\n",
      "Sample 8:\n",
      "Original Dialogue: Rita: I'm so bloody tired. Falling asleep at work. :-(\n",
      "Tina: I know what you mean.\n",
      "Tina: I keep on nodding off at my keyboard hoping that the boss doesn't notice..\n",
      "Rita: The time just keeps on dragging on and on and on.... \n",
      "Rita: I keep on looking at the clock and there's still 4 hours of this drudgery to go.\n",
      "Tina: Times like these I really hate my work.\n",
      "Rita: I'm really not cut out for this level of boredom.\n",
      "Tina: Neither am I.\n",
      "Model Summary    : Rita is tired and falling asleep at work. Tina is bored.\n",
      "Reference Summary: Rita and Tina are bored at work and have still 4 hours left.\n",
      "\n",
      "Sample 9:\n",
      "Original Dialogue: Beatrice: I am in town, shopping. They have nice scarfs in the shop next to the church. Do you want one?\n",
      "Leo: No, thanks\n",
      "Beatrice: But you don't have a scarf.\n",
      "Leo: Because I don't need it.\n",
      "Beatrice: Last winter you had a cold all the time. A scarf could help.\n",
      "Leo: I don't like them.\n",
      "Beatrice: Actually, I don't care. You will get a scarf.\n",
      "Leo: How understanding of you!\n",
      "Beatrice: You were complaining the whole winter that you're going to die. I've had enough.\n",
      "Leo: Eh.\n",
      "Model Summary    : Leo doesn't want a scarf because he had a cold all the time last winter.\n",
      "Reference Summary: Beatrice wants to buy Leo a scarf, but he doesn't like scarves. She cares about his health and will buy him a scarf no matter his opinion.\n",
      "\n",
      "Sample 10:\n",
      "Original Dialogue: Ivan: hey eric\n",
      "Eric: yeah man\n",
      "Ivan: so youre coming to the wedding\n",
      "Eric: your brother's\n",
      "Ivan: yea\n",
      "Eric: i dont know mannn\n",
      "Ivan: YOU DONT KNOW??\n",
      "Eric: i just have a lot to do at home, plus i dont know if my parents would let me\n",
      "Ivan: ill take care of your parents\n",
      "Eric: youre telling me you have the guts to talk to them XD\n",
      "Ivan: thats my problem\n",
      "Eric: okay man, if you say so\n",
      "Ivan: yea just be there \n",
      "Eric: alright\n",
      "Model Summary    : Eric is coming to Ivan's brother's wedding. Eric doesn't know if his parents will let him come.\n",
      "Reference Summary: Eric doesn't know if his parents let him go to Ivan's brother's wedding. Ivan will talk to them.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"transformersbook/pegasus-samsum\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"transformersbook/pegasus-samsum\")\n",
    "\n",
    "# Load test data from test.json\n",
    "with open(\"test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# Use only the first 10 dialogues\n",
    "samples = data[:10]\n",
    "\n",
    "# Generate and print summaries\n",
    "for i, sample in enumerate(samples):\n",
    "    dialogue = sample[\"dialogue\"]\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(dialogue, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=60,\n",
    "        min_length=10,\n",
    "        num_beams=4,\n",
    "        length_penalty=2.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(\"Original Dialogue:\", dialogue)\n",
    "    print(\"Model Summary    :\", summary)\n",
    "    print(\"Reference Summary:\", sample[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cdb171",
   "metadata": {},
   "source": [
    "# Using Pipeline for Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2ca81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vmcsa\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "Original Dialogue: Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "Model Summary    : Amanda can't find Betty's number. Larry called her last time they were at the park together.\n",
      "Reference Summary: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "\n",
      "Sample 2:\n",
      "Original Dialogue: Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it's really funny!\n",
      "Eric: I know! I especially like the train part!\n",
      "Rob: Hahaha! No one talks to the machine like that!\n",
      "Eric: Is this his only stand-up?\n",
      "Rob: Idk. I'll check.\n",
      "Eric: Sure.\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\n",
      "Eric: Gr8! I'll watch them now!\n",
      "Rob: Me too!\n",
      "Eric: MACHINE!\n",
      "Rob: MACHINE!\n",
      "Eric: TTYL?\n",
      "Rob: Sure :)\n",
      "Model Summary    : Eric, Rob and Rob will watch the Russian comedian's stand-up.\n",
      "Reference Summary: Eric and Rob are going to watch a stand-up on youtube.\n",
      "\n",
      "Sample 3:\n",
      "Original Dialogue: Lenny: Babe, can you help me with something?\n",
      "Bob: Sure, what's up?\n",
      "Lenny: Which one should I pick?\n",
      "Bob: Send me photos\n",
      "Lenny:  <file_photo>\n",
      "Lenny:  <file_photo>\n",
      "Lenny:  <file_photo>\n",
      "Bob: I like the first ones best\n",
      "Lenny: But I already have purple trousers. Does it make sense to have two pairs?\n",
      "Bob: I have four black pairs :D :D\n",
      "Lenny: yeah, but shouldn't I pick a different color?\n",
      "Bob: what matters is what you'll give you the most outfit options\n",
      "Lenny: So I guess I'll buy the first or the third pair then\n",
      "Bob: Pick the best quality then\n",
      "Lenny: ur right, thx\n",
      "Bob: no prob :)\n",
      "Model Summary    : Lenny will buy the first or the third pair of purple trousers from Bob.\n",
      "Reference Summary: Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.\n",
      "\n",
      "Sample 4:\n",
      "Original Dialogue: Will: hey babe, what do you want for dinner tonight?\n",
      "Emma:  gah, don't even worry about it tonight\n",
      "Will: what do you mean? everything ok?\n",
      "Emma: not really, but it's ok, don't worry about cooking though, I'm not hungry\n",
      "Will: Well what time will you be home?\n",
      "Emma: soon, hopefully\n",
      "Will: you sure? Maybe you want me to pick you up?\n",
      "Emma: no no it's alright. I'll be home soon, i'll tell you when I get home. \n",
      "Will: Alright, love you. \n",
      "Emma: love you too. \n",
      "Model Summary    : Emma will be home soon. She will tell Will when she gets home.\n",
      "Reference Summary: Emma will be home soon and she will let Will know.\n",
      "\n",
      "Sample 5:\n",
      "Original Dialogue: Ollie: Hi , are you in Warsaw\n",
      "Jane: yes, just back! Btw are you free for diner the 19th?\n",
      "Ollie: nope!\n",
      "Jane: and the  18th?\n",
      "Ollie: nope, we have this party and you must be there, remember?\n",
      "Jane: oh right! i lost my calendar..  thanks for reminding me\n",
      "Ollie: we have lunch this week?\n",
      "Jane: with pleasure!\n",
      "Ollie: friday?\n",
      "Jane: ok\n",
      "Jane: what do you mean \" we don't have any more whisky!\" lol..\n",
      "Ollie: what!!!\n",
      "Jane: you just call me and the all thing i heard was that sentence about whisky... what's wrong with you?\n",
      "Ollie: oh oh... very strange! i have to be carefull may be there is some spy in my mobile! lol\n",
      "Jane: dont' worry, we'll check on friday.\n",
      "Ollie: don't forget to bring some sun with you\n",
      "Jane: I can't wait to be in Morocco..\n",
      "Ollie: enjoy and see you friday\n",
      "Jane: sorry Ollie, i'm very busy, i won't have time for lunch  tomorrow, but may be at 6pm after my courses?this trip to Morocco was so nice, but time consuming!\n",
      "Ollie: ok for tea!\n",
      "Jane: I'm on my way..\n",
      "Ollie: tea is ready, did you bring the pastries?\n",
      "Jane: I already ate them all... see you in a minute\n",
      "Ollie: ok\n",
      "Model Summary    : Jane is in Warsaw. She lost her calendar. She's on her way to Morocco.\n",
      "Reference Summary: Jane is in Warsaw. Ollie and Jane has a party. Jane lost her calendar. They will get a lunch this week on Friday. Ollie accidentally called Jane and talked about whisky. Jane cancels lunch. They'll meet for a tea at 6 pm.\n",
      "\n",
      "Sample 6:\n",
      "Original Dialogue: Benjamin: Hey guys, what are we doing with the keys today?\n",
      "Hilary: I've got them. Whoever wants them can meet me at lunchtime or after\n",
      "Elliot: I'm ok. We're meeting for the drinks in the evening anyway and I guess we'll be going back to the apartment together?\n",
      "Hilary: Yeah, I guess so\n",
      "Daniel: I'm with Hilary atm and won't let go of her for the rest of the day, so any option you guys choose is good for me\n",
      "Benjamin: Hmm I might actually pass by at lunchtime, take the keys and go take a nap. I'm sooo tired after yesterday\n",
      "Hilary: Sounds good. We'll be having lunch with some French people (the ones who work on the history of food in colonial Mexico - I already see you yawning your head off)\n",
      "Benjamin: YAAAAWN 🙊 Where and where are you meeting?\n",
      "Hilary: So I'm meeting them at the entrance to the conference hall at 2 pm and then we'll head to this place called La Cantina. Italian cuisine, which is quite funny, but that's what they've chosen\n",
      "Benjamin: Interesting 😱 To be honest, Hilary, I almost feel like changing my mind. Wanting to take this nap might end up costing me to dear\n",
      "Hilary: Oh come on 😂\n",
      "Benjamin: All these terrible obstacles on mu way to bed might just prove to much to take\n",
      "Hilary: We'll try to avoid talking about their subject of research. Oh wait, no, I'm actually meeting them because I wanted to chat about their research lol\n",
      "Elliot: 🙉\n",
      "Hilary: Do join us, we're going to have fun. And then you'll take the keys and take this most deserved of naps\n",
      "Elliot: Sounds like a plan 😂\n",
      "Hilary: 😎\n",
      "Elliot: See you at 2 then xx\n",
      "Model Summary    : Benjamin, Elliot, Hilary and Daniel are meeting at La Cantina at 2 pm. They're going to have lunch with French people who work on the history of food in colonial Mexico. Hilary is meeting them at the entrance to the conference hall at 2 pm and then they'll head to La\n",
      "Reference Summary: Hilary has the keys to the apartment. Benjamin wants to get them and go take a nap. Hilary is having lunch with some French people at La Cantina. Hilary is meeting them at the entrance to the conference hall at 2 pm. Benjamin and Elliot might join them. They're meeting for the drinks in the evening.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 113. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=56)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 7:\n",
      "Original Dialogue: Max: Know any good sites to buy clothes from?\n",
      "Payton: Sure :) <file_other> <file_other> <file_other> <file_other> <file_other> <file_other> <file_other>\n",
      "Max: That's a lot of them!\n",
      "Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\n",
      "Max: I'll check them out. Thanks. \n",
      "Payton: No problem :)\n",
      "Max: How about u?\n",
      "Payton: What about me?\n",
      "Max: Do u like shopping?\n",
      "Payton: Yes and no.\n",
      "Max: How come?\n",
      "Payton: I like browsing, trying on, looking in the mirror and seeing how I look, but not always buying.\n",
      "Max: Y not?\n",
      "Payton: Isn't it obvious? ;)\n",
      "Max: Sry ;)\n",
      "Payton: If I bought everything I liked, I'd have nothing left to live on ;)\n",
      "Max: Same here, but probably different category ;)\n",
      "Payton: Lol\n",
      "Max: So what do u usually buy?\n",
      "Payton: Well, I have 2 things I must struggle to resist!\n",
      "Max: Which are?\n",
      "Payton: Clothes, ofc ;)\n",
      "Max: Right. And the second one?\n",
      "Payton: Books. I absolutely love reading!\n",
      "Max: Gr8! What books do u read?\n",
      "Payton: Everything I can get my hands on :)\n",
      "Max: Srsly?\n",
      "Payton: Yup :)\n",
      "Model Summary    : Max will check out some good sites to buy clothes from. Payton will buy clothes and books.\n",
      "Reference Summary: Payton provides Max with websites selling clothes. Payton likes browsing and trying on the clothes but not necessarily buying them. Payton usually buys clothes and books as he loves reading.\n",
      "\n",
      "Sample 8:\n",
      "Original Dialogue: Rita: I'm so bloody tired. Falling asleep at work. :-(\n",
      "Tina: I know what you mean.\n",
      "Tina: I keep on nodding off at my keyboard hoping that the boss doesn't notice..\n",
      "Rita: The time just keeps on dragging on and on and on.... \n",
      "Rita: I keep on looking at the clock and there's still 4 hours of this drudgery to go.\n",
      "Tina: Times like these I really hate my work.\n",
      "Rita: I'm really not cut out for this level of boredom.\n",
      "Tina: Neither am I.\n",
      "Model Summary    : Rita is tired and falling asleep at work. Tina hates her work.\n",
      "Reference Summary: Rita and Tina are bored at work and have still 4 hours left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 111. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=55)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 9:\n",
      "Original Dialogue: Beatrice: I am in town, shopping. They have nice scarfs in the shop next to the church. Do you want one?\n",
      "Leo: No, thanks\n",
      "Beatrice: But you don't have a scarf.\n",
      "Leo: Because I don't need it.\n",
      "Beatrice: Last winter you had a cold all the time. A scarf could help.\n",
      "Leo: I don't like them.\n",
      "Beatrice: Actually, I don't care. You will get a scarf.\n",
      "Leo: How understanding of you!\n",
      "Beatrice: You were complaining the whole winter that you're going to die. I've had enough.\n",
      "Leo: Eh.\n",
      "Model Summary    : Leo doesn't need a scarf because he had a cold all the time last winter. Beatrice will buy him one.\n",
      "Reference Summary: Beatrice wants to buy Leo a scarf, but he doesn't like scarves. She cares about his health and will buy him a scarf no matter his opinion.\n",
      "\n",
      "Sample 10:\n",
      "Original Dialogue: Ivan: hey eric\n",
      "Eric: yeah man\n",
      "Ivan: so youre coming to the wedding\n",
      "Eric: your brother's\n",
      "Ivan: yea\n",
      "Eric: i dont know mannn\n",
      "Ivan: YOU DONT KNOW??\n",
      "Eric: i just have a lot to do at home, plus i dont know if my parents would let me\n",
      "Ivan: ill take care of your parents\n",
      "Eric: youre telling me you have the guts to talk to them XD\n",
      "Ivan: thats my problem\n",
      "Eric: okay man, if you say so\n",
      "Ivan: yea just be there \n",
      "Eric: alright\n",
      "Model Summary    : Eric is coming to Ivan's brother's wedding. Eric doesn't know if his parents will let him come.\n",
      "Reference Summary: Eric doesn't know if his parents let him go to Ivan's brother's wedding. Ivan will talk to them.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "# Load the summarization pipeline for pegasus-samsum\n",
    "summarizer = pipeline(\"summarization\", model=\"transformersbook/pegasus-samsum\")\n",
    "\n",
    "# Load the JSON file (test.json)\n",
    "with open(\"test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# Take first 10 samples\n",
    "samples = data[:10]\n",
    "\n",
    "# Run summarization\n",
    "for i, sample in enumerate(samples):\n",
    "    dialogue = sample[\"dialogue\"]\n",
    "    summary = summarizer(\n",
    "        dialogue,\n",
    "        max_new_tokens=60,    # Use max_new_tokens instead of max_length\n",
    "        min_length=15,\n",
    "        do_sample=False,\n",
    "        truncation=True\n",
    "    )[0][\"summary_text\"]\n",
    "\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(\"Original Dialogue:\", dialogue)\n",
    "    print(\"Model Summary    :\", summary)\n",
    "    print(\"Reference Summary:\", sample[\"summary\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
